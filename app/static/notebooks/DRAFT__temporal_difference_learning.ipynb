{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Temporal difference learning\n",
    "\n",
    "I recently read a historic article, \"Learning to Predict by the Methods of Temporal Differences\" by Richard Sutton, dated 1988.  While temporal difference (TD) had already been previously used--mostly in an empirical fashion--, this document is among the first to develop a formal theoretical analysis of TD performance, convergence and optimality properties.  This post is mostly rehash of the original article, which I suggest everybody interested in the subject matter to read.\n",
    "\n",
    "I want to make a special effort to highlight a fundamental misunderstanding I suffered from regarding TD until reading this article.  Briefly said, I was quite surprised that TD was analysed outside of the reinforcement learning (RL) framework, but rather within that of supervised learning (SL).  In fact, RL is not mentioned at all throughout the whole article!  This misunderstanding is in my opinion simply explained by the fact that, in modern literature, TD is only ever introduced within the context of sequential decision making.  However, the key insight is that, even within the RL framework, TD is used to perform value **prediction** and not **decision making**.  As such, TD is As such, TD has much more in common with SL algorithms than with RL ones.\n",
    "\n",
    "\n",
    "### Sequential prediction problems\n",
    "\n",
    "The methods of temporal difference are applicable in sequential prediction problems, where multiple rounds of information can be obtained before observing the actual target.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the document, TD is introduced as a strategy for making incremental predictions in dynamic settings, but more importantly, *as an alternative strategy to supervised learning*.  While this should perhaps not be a surprise, it sure felt like one to me.  The argument is very compelling:  even in the context of reinforcement learning, TD is used to learn value functions, which are nothing more than predictors of expected returns.  The learning process does not inherently depend on the decision process--in fact, it is directly applicable to Markov reward processes.  This perhaps (but perhaps not) subtle distinction eluded me before reading this document.  In my experience, TD has only been introduced in treatments of reinforcement learning, and thus was ingrained in my brain the notion that TD was a specialized technique to solve decision processes.\n",
    "\n",
    "$ \\mathcal{L}(f, y) = \\frac{1}{2}\\left(f - y\\right)^2 $\n",
    "\n",
    "$ \\mathcal{L}_t(\\beta_t) = \\mathcal{L}(f_t, y) $\n",
    "\n",
    "$ \\mathcal{L}_t(\\beta_t) = \\mathcal{L}(f_t, f_{t+1}) $\n",
    "\n",
    "It is perhaps surprising that learning **improves** if we use a biased target $f_{t+1}$ instead of the correct one $y$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Temporal Difference Learning\n",
    "\n",
    "Although temporal difference strategies had been previously empirically employed, this document represents \n",
    "\n",
    "Introduces dynamic learning domains as those where a sequence of observations ($x_1, x_2, \\ldots$) are made before the target variable $y$ is revealed.  Sutton argues that many real world domains reflect this structure:  weather forecast, economic investment, most games, etc..\n",
    "\n",
    "Solving the learning problem in dynamic domains means to construct a series of predictors $f_t$, one for each timestep.\n",
    "\n",
    "Supervised learning addresses this by learning each predictor $f_t = x_t^\\top\\beta_t$ independently on a separate dataset $\\{(x_t^{(n)}, y^{(n)})\\}_{i=1}^n$.  This straightforward and natural approach suffers from some issues.  Most notably, one needs to keep track of all observations until the final target is revealed.  This may become an issue in domains where the chain of observations are long:  Each observation needs to be stored in memory until the target variable is revealed, and only then can each model be improved.\n",
    "\n",
    "In contrast, temporal difference learning exploits the fact that future predictions are typically more precise than current predictions.\n",
    "\n",
    "Supervised learning trains models by comparing predictions to targets, while temporal difference learning trains models by comparing current predictions to future predictions.\n",
    "\n",
    "\n",
    "This type of learning problem can be addressed by simple supervised learning, where each \n",
    "\n",
    "### Random walk domain\n",
    "\n",
    " * state starts out at the origin of a d-dimensional Euclidean space, and thereafter n random movements are performed.  The position after each movement represents each timestep's state.  At the end of the walk, the l_2 distance from the origin is taken to be the regression target.\n",
    " \n",
    " This toy system satisfies an important property of real dynamic systems:  as the random steps are performed, the current state becomes more indicative of the final target.\n",
    " \n",
    " (just like in go, it is simpler to predict the wier during the endgame than during the opening phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00116523  -0.06507548]\n",
      " [  2.34871636   0.09759921]\n",
      " [  3.65100767  -0.10547027]\n",
      " [  4.57363336  -0.48313131]\n",
      " [  5.8209196   -0.0628875 ]\n",
      " [  6.84365999   0.02076998]\n",
      " [  8.2050835    0.1002438 ]\n",
      " [  9.0169544    0.28686697]\n",
      " [ 10.40533771   0.10053526]\n",
      " [ 11.84856735   0.07368175]]\n",
      "11.8487964428\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "class random_walk_problem(object):\n",
    "    def __init__(self, mean, cov, nsteps):\n",
    "        self.step_kernel = multivariate_normal(mean, cov)\n",
    "        self.nsteps = nsteps\n",
    "    \n",
    "    def walk(self):\n",
    "        pos = 0\n",
    "        for i in xrange(self.nsteps):\n",
    "            pos += self.step_kernel.rvs()\n",
    "            yield pos.copy()\n",
    "    \n",
    "    def walk_list(self):\n",
    "        return list(self.walk())\n",
    "    \n",
    "    def walk_array(self):\n",
    "        return np.array(self.walk_list())\n",
    "\n",
    "    @staticmethod\n",
    "    def target(walk):\n",
    "        return la.norm(walk[-1])\n",
    "\n",
    "ndim = 2\n",
    "mean = np.eye(ndim)[0]\n",
    "cov = .1 * np.eye(ndim)\n",
    "rwalk = random_walk_problem(mean, cov, 10)\n",
    "\n",
    "walk = rwalk.walk_array()\n",
    "y = rwalk.target(walk)\n",
    "print walk\n",
    "print y\n",
    "\n",
    "\n",
    "\n",
    "class SequenceModel(object):\n",
    "    def __init__(self, nsteps):\n",
    "        self.nsteps = nsteps\n",
    "        self.betas = None\n",
    "    \n",
    "    def f_t(self, x, t):\n",
    "        return np.dot(x, self.betas[t])\n",
    "        \n",
    "    def f(self, X):\n",
    "        return [self.f_t(x, t) for t, x in enumerate(X)]\n",
    "\n",
    "def loss(self, f, y):\n",
    "    return .5 * (f - y) ** 2\n",
    "\n",
    "def dloss(self, f, y, df):\n",
    "    return (f - y) * df\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.beta is None:\n",
    "            self.beta = np.zeros(len(x))\n",
    "        return np.dot(x, self.beta)\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        \n",
    "\n",
    "def make_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
